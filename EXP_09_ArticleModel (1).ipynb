{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOn3JnsAvdUtb0Vm2H+dM2i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jYPpTUVX-kyy","outputId":"7a19c94e-075c-432e-d79b-a76c2317c9c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n","\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 172ms/step - accuracy: 0.3446 - loss: 2.9455 - val_accuracy: 0.4869 - val_loss: 2.0535\n","Epoch 2/5\n","\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 167ms/step - accuracy: 0.4995 - loss: 2.0464 - val_accuracy: 0.4808 - val_loss: 2.0602\n","Epoch 3/5\n","\u001b[1m 36/113\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 132ms/step - accuracy: 0.5121 - loss: 2.0084"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import reuters\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","\n","(num_words) = 10000\n","(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words)\n","\n","max_len = 200\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen=max_len)\n","\n","num_classes = max(y_train) + 1\n","\n","model = Sequential([\n","    Embedding(num_words, 64, input_length=max_len),  # word2vec-like embeddings\n","    LSTM(64),\n","    Dense(num_classes, activation='softmax')         # multi-class output\n","])\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","history = model.fit(\n","    x_train, y_train,\n","    epochs=5,\n","    batch_size=64,\n","    validation_split=0.2,\n","    verbose=1\n",")\n","loss, acc = model.evaluate(x_test, y_test, verbose=0)\n","print(f\"Test Accuracy: {acc*100:.2f}%\")\n"]}]}